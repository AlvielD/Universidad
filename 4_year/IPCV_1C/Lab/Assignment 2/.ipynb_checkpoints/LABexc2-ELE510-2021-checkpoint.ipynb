{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# ELE510 Image Processing with robot vision: LAB, Exercise 2, Image Formation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "**Purpose:** *To learn about the image formation process, i.e. how images are projected from the scene to the image plane.*\n",
    "\n",
    "The theory for this exercise can be found in chapter 2 and 3 of the text book [1]. Supplementary information can found in chapter 1, 2 and 3 in the compendium [2]. See also the following documentations for help:\n",
    "- [OpenCV](https://opencv.org/opencv-python-free-course/)\n",
    "- [numpy](https://numpy.org/doc/stable/)\n",
    "- [matplotlib](https://matplotlib.org/stable/contents.html)\n",
    "\n",
    "**IMPORTANT:** Read the text carefully before starting the work. In\n",
    "many cases it is necessary to do some preparations before you start the work\n",
    "on the computer. Read necessary theory and answer the theoretical part\n",
    "frst. The theoretical and experimental part should be solved individually.\n",
    "The notebook must be approved by the lecturer or his assistant.\n",
    "\n",
    "**Approval:**\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "The current notebook should be submitted on CANVAS as a single pdf file. \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    To export the notebook in a pdf format, goes to File -> Download as -> PDF via LaTeX (.pdf).\n",
    "</div>\n",
    "\n",
    "**Note regarding the notebook**: The theoretical questions can be answered directly on the notebook using a *Markdown* cell and LaTex commands (if relevant). In alternative, you can attach a scan (or an image) of the answer directly in the cell.\n",
    "\n",
    "Possible ways to insert an image in the markdown cell:\n",
    "\n",
    "`![image name](\"image_path\")`\n",
    "\n",
    "`<img src=\"image_path\" alt=\"Alt text\" title=\"Title text\" />`\n",
    "\n",
    "\n",
    "**Under you will find parts of the solution that is already programmed.**\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p>You have to fill out code everywhere it is indicated with `...`</p>\n",
    "    <p>The code section under `######## a)` is answering subproblem a) etc.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "**a)** What is the meaning of the abbreviation PSF? What does the PSF specify?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PSF means Point Spread Function and it specifies the shape that a point will take\n",
    "on the image plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "**b)** Use the imaging model shown in Figure 1. The camera has a lens with focal length $f = 40\\text{mm}$ and in the image plane a CCD sensor of size $8\\text{mm} \\times 8\\text{mm}$. The total number of pixels is $4000 \\times 4000$. How many lines per mm will this camera resolve at a distance of $z_w = 1\\text{m}$ from the camera center?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<img src=\"./images/perspectiveProjection.jpg\" alt=\"Alt text\" title=\"Title text\" />\n",
    "\n",
    "**Figure 1**: Perspective projection caused by a pinhole camera. Figure 2.23 in [2].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution to the exercise:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image name](images/Problem_1.b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "**c)** Explain how a Bayer filter works. What is the alternative to using this type of filter in image acquisition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bayer color filter is formed by 3 raws, each has a primary color, but with in-between pixels missing, to obtain the full color information we use debayering algorithms which interpolates the missing information from neighboring pixels and provide estimation. An alternative for this could be using 3 different filters to catch every color and then combining them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "## Problem 2\n",
    "\n",
    "Assume we have captured an image with a digital camera. The image covers an area in the scene of size $1.024\\text{m} \\times 0.768\\text{m}$ (The camera has been pointed towards a wall such that the distance is approximately constant over the whole image plane, *weak perspective*). The camera has 2048 pixels horizontally, and 1536 pixels vertically. The active region on the CCD-chip is $10\\text{mm} \\times 7.5\\text{mm}$. We define the spatial coordinates $(x_w,y_w)$ such that the origin is at the center of the optical axis, x-axis horizontally and y-axis vertically upwards. The image indexes $(x,y)$ is starting in the upper left corner. For simplicity let the optical axis meet the image plane at $(x_{0}=1024,y_{0}=768)$. The solutions to this problem can be found from simple geometric considerations. Make a sketch of the situation and answer the following questions:\n",
    "\n",
    "**a)** What is the size of each sensor (one pixel) on the CCD-chip?\n",
    "\n",
    "**b)** What is the scaling coefficient between the image plane (CCD-chip) and the scene? What is the scaling coefficient between the scene coordinates and the image indexes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution to the section a)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image name](images/Problem_2.a.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution to the section b)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image name](images/Problem_2.b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "## Problem 3\n",
    "\n",
    "Translation from the scene to a camera sensor can be done using a transformation matrix, $T$. \n",
    "\n",
    "\\begin{equation}\n",
    "\t\\begin{bmatrix} x\\\\y\\\\1\\end{bmatrix} = \n",
    "\tT\n",
    "\t\\begin{bmatrix}\n",
    "\t\tx_w\\\\ y_w\\\\ 1\n",
    "\t\\end{bmatrix}\\\\\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "\tT= \\begin{bmatrix} \\alpha_x & 0 & x_0\\\\\n",
    "\t\t\t0 & \\alpha_y & y_0\\\\\n",
    "\t\t0   & 0 & 1\n",
    "\t\\end{bmatrix}\n",
    "\\end{equation}\n",
    "$\\alpha_x$ and $\\alpha_y$ are the scaling factors for their corresponding axes.\n",
    "\n",
    "Write a function in Python that computes the image points using the transformation matrix, using the parameters from Problem 2. Let the input to the function be a set of $K$ scene points, given by a $2 \\times K$ matrix, and the output the resulting image points also given by a $2 \\times K$ matrix. The parameters defining the image sensor and field of view from the camera center to the wall can also be given as input parameters.\n",
    "\n",
    "Test the function for the following input points given as a matrix:\n",
    "\\begin{equation}\\label{cam-eq4}\n",
    "    {\\mathbf P}_{in} = \\begin{bmatrix} 0.512 & -0.512 & -0.512 & 0.512 & 0 & 0.3 & 0.3 & 0.3 & 0.6 \\\\\n",
    "    0.384 & 0.384 & -0.384 & -0.384 & 0 & 0.2 & -0.2 & -0.4 & 0 \\end{bmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Comment on the results, especially notice the two last points!\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages that are useful inside the definition of the weakPerspective function\n",
    "import math \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that takes in input:\n",
    "- FOV: field of view,\n",
    "- sensorsize: size of the sensor,\n",
    "- n_pixels: camera pixels,\n",
    "- p_scene: K input points (2xK matrix)\n",
    "\n",
    "and return the resulting image points given the 2xK matrix\n",
    "\"\"\"\n",
    "def weakPerspective(FOV, sensorsize, n_pixels, p_scene):\n",
    "    \n",
    "    # Get an array of the same size of p_scene\n",
    "    p_out = np.zeros(np.shape(p_scene))\n",
    "    \n",
    "    ## Get the scaling coefficient for the x axis\n",
    "    width = sensorsize*n_pixels[0]   # Width of the active region\n",
    "    \n",
    "    coef_x = ((width / FOV[0]) / sensorsize)\n",
    "    \n",
    "    ## Get the scaling coefficient for the y axis\n",
    "    heigth = sensorsize*n_pixels[1]  # Heigth of the active region\n",
    "    \n",
    "    coef_y = ((heigth / FOV[1]) / sensorsize)\n",
    "    \n",
    "    T = np.array([[coef_x, 0, n_pixels[0]/2], [0, coef_y, n_pixels[1]/2], [0, 0, 1]])\n",
    "    \n",
    "    row = np.ones(np.shape(p_scene)[1])\n",
    "    p_scene = np.append(p_scene, [row], axis=0)\n",
    "    \n",
    "    p_out = np.matmul(T, p_scene)\n",
    "        \n",
    "    return p_out[0:2, :].astype(int)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above function is then called using the following parameters:\n",
    "\n",
    "# Parameters\n",
    "FOV = [1.024, 0.768]\n",
    "sensorsize = 4.88*10**-3\n",
    "n_pixels = [2048, 1536]\n",
    "p_scene_x = [0.512, -0.512, -0.512, 0.512, 0, 0.3, 0.3, 0.3, 0.6]\n",
    "p_scene_y = [0.384, 0.384, -0.384, -0.384, 0, 0.2, -0.2, -0.4, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2048    0    0 2048 1024 1624 1624 1624 2224]\n",
      " [1536 1536    0    0  768 1168  368  -32  768]]\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# This cell is locked; it can be only be executed to see the results. \n",
    "####\n",
    "# Input data:\n",
    "p_scene = np.array([p_scene_x, p_scene_y])\n",
    "\n",
    "# Call to the weakPerspective() function \n",
    "pimage = weakPerspective(FOV, sensorsize, n_pixels, p_scene)\n",
    "\n",
    "# Result: \n",
    "print(pimage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the last to points are out of the image plane, we can notice this cause the y coordinate of the 8th point is -32 (this value is less than 0 so it is out of the FOV) and the x coordinate of the last point is 2224 (this value is greater than 2048, also out of the FOV).\n",
    "We could solve this in our function, if one of the coordinates is out of the FOV, we could throw an exception or control it somehow (printing a message or ignoring it for example.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "\n",
    "### Delivery (dead line) on CANVAS: 12-09-2021 at 23:59\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Contact\n",
    "### Course teacher\n",
    "Professor Kjersti Engan, room E-431\n",
    "\n",
    "E-mail: kjersti.engan@uis.no\n",
    "\n",
    "### Teaching assistant\n",
    "Tomasetti Luca, room E-401\n",
    "\n",
    "E-mail: luca.tomasetti@uis.no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "## References\n",
    "\n",
    "[1] S. Birchfeld, Image Processing and Analysis. Cengage Learning, 2016.\n",
    "\n",
    "[2] I. Austvoll, \"Machine/robot vision part I,\" University of Stavanger, 2018. Compendium, CANVAS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
